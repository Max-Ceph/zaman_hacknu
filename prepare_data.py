import json
import os
from dotenv import load_dotenv
from openai import OpenAI
import time

# --- 1. –ù–ê–°–¢–†–û–ô–ö–ê API-–ö–õ–ò–ï–ù–¢–ê ---
load_dotenv()

openai_api_key = os.getenv("OPENAI_API_KEY")

if not openai_api_key:
    raise ValueError("‚ùå OPENAI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env —Ñ–∞–π–ª–µ!")

openai_client = OpenAI(api_key=openai_api_key)
print("‚úì OpenAI API –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

# --- 2. –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –Ø–ó–´–ö–û–í ---
# –ó–¥–µ—Å—å –º—ã —É–∫–∞–∑—ã–≤–∞–µ–º, –∫–∞–∫–∏–µ —Ñ–∞–π–ª—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å.
# –í—ã –º–æ–∂–µ—Ç–µ –¥–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—ã–µ —è–∑—ã–∫–∏, –ø—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤–∏–≤ –Ω–æ–≤—É—é —Å—Ç—Ä–æ–∫—É.
LANGUAGES = [
    {
        "name": "–†—É—Å—Å–∫–∏–π",
        "input_file": "knowledge_base.json",
        "output_file": "vector_database.json"
    },
    {
        "name": "–ö–∞–∑–∞—Ö—Å–∫–∏–π",
        "input_file": "knowledge_base_kk.json",
        "output_file": "vector_database_kk.json"
    }
]


# --- 3. –§–£–ù–ö–¶–ò–ò –î–õ–Ø –û–ë–†–ê–ë–û–¢–ö–ò –î–ê–ù–ù–´–• ---

def load_knowledge_base(filename):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞."""
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"‚ùå –û—à–∏–±–∫–∞: –§–∞–π–ª {filename} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        print(f"   –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏.")
        return None


def get_embedding(text_chunk, model="text-embedding-3-small"):
    """–ü–æ–ª—É—á–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é API."""
    try:
        text_chunk = text_chunk.replace("\n", " ")
        response = openai_client.embeddings.create(input=[text_chunk], model=model)
        return response.data[0].embedding
    except Exception as e:
        print(f"  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏: {e}")
        time.sleep(5)
        return None


def chunk_text(text, chunk_size=1000, chunk_overlap=200):
    """–†–∞–∑–±–∏–≤–∞–µ—Ç –±–æ–ª—å—à–æ–π —Ç–µ–∫—Å—Ç –Ω–∞ –ø–µ—Ä–µ—Å–µ–∫–∞—é—â–∏–µ—Å—è —á–∞–Ω–∫–∏."""
    if len(text) <= chunk_size:
        return [text]
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start += chunk_size - chunk_overlap
    return chunks


# --- 4. –û–°–ù–û–í–ù–ê–Ø –õ–û–ì–ò–ö–ê –û–ë–†–ê–ë–û–¢–ö–ò ---

def process_language_files(lang_config):
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ —è–∑—ã–∫–∞:
    –∑–∞–≥—Ä—É–∑–∫–∞ -> —á–∞–Ω–∫–∏–Ω–≥ -> –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è -> —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ.
    """
    input_file = lang_config["input_file"]
    output_file = lang_config["output_file"]
    lang_name = lang_config["name"]

    print("\n" + "=" * 60)
    print(f"–ù–ê–ß–ê–¢–ê –û–ë–†–ê–ë–û–¢–ö–ê –î–õ–Ø –Ø–ó–´–ö–ê: {lang_name.upper()}")
    print(f"–í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª: {input_file}")
    print("=" * 60 + "\n")

    knowledge_data = load_knowledge_base(input_file)
    if not knowledge_data:
        return  # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É —è–∑—ã–∫—É, –µ—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω

    vector_database = []
    total_chunks = 0

    print(f"üìö –ù–∞–π–¥–µ–Ω–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(knowledge_data)}")
    print("üîÑ –ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É...\n")

    for idx, entry in enumerate(knowledge_data, 1):
        source = entry["source_url"]
        content = entry["content"]
        chunks = chunk_text(content)

        print(f"[{idx}/{len(knowledge_data)}] –û–±—Ä–∞–±–æ—Ç–∫–∞: {source}")
        print(f"           –ß–∞–Ω–∫–æ–≤: {len(chunks)}")

        for i, chunk in enumerate(chunks):
            print(f"           ‚îî‚îÄ –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è {i + 1}/{len(chunks)}...", end=" ")
            vector = get_embedding(chunk)

            if vector:
                vector_database.append({
                    "source": source,
                    "content": chunk,
                    "vector": vector
                })
                total_chunks += 1
                print("‚úì")
            else:
                print("‚ùå")

            time.sleep(0.5)

        print()

    if not vector_database:
        print(f"\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è {lang_name}. –§–∞–π–ª –Ω–µ –±—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω.")
        return

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(vector_database, f, ensure_ascii=False, indent=4)

    print(f"‚úÖ –û–ë–†–ê–ë–û–¢–ö–ê –î–õ–Ø '{lang_name}' –ó–ê–í–ï–†–®–ï–ù–ê!")
    print(f"üì¶ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_file}")
    print(f"üìä –í—Å–µ–≥–æ –≤–µ–∫—Ç–æ—Ä–æ–≤: {total_chunks}")


# --- 5. –ó–ê–ü–£–°–ö –°–ö–†–ò–ü–¢–ê ---

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: –∑–∞–ø—É—Å–∫–∞–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–ª—è –≤—Å–µ—Ö —è–∑—ã–∫–æ–≤, —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –≤ LANGUAGES."""
    print("\n" + "#" * 60)
    print("–ó–ê–ü–£–°–ö –°–ö–†–ò–ü–¢–ê –ü–û–î–ì–û–¢–û–í–ö–ò –í–ï–ö–¢–û–†–ù–´–• –ë–ê–ó –î–ê–ù–ù–´–•")
    print("#" * 60)

    for lang_config in LANGUAGES:
        process_language_files(lang_config)

    print("\n" + "=" * 60)
    print("üéâ –í–°–ï –û–ü–ï–†–ê–¶–ò–ò –ó–ê–í–ï–†–®–ï–ù–´!")
    print("=" * 60 + "\n")


if __name__ == "__main__":
    main()